<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Spike Train Dataset Conversion Tool for Spiking Neural Network Analysis</title>
  
  <!-- Bootstrap -->
  <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
  <link href="css/custom.css" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">

  <link href="css/style.css" rel="stylesheet" type="text/css" media="all" />
  <!-- Bootstrap core CSS -->
  <link href="css/css/bootstrap.min.css" rel="stylesheet">


  <style>
    body {
      background: rgb(255, 255, 255) no-repeat fixed top left;
      font-family: 'Open Sans', sans-serif;
    }
  </style>

</head>
<!-- cover -->
<section>
  <div class="jumbotron text-center mt-0">
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2>Spike Train Dataset Conversion Tool for Spiking Neural Network Analysis
          </h2>
          <!-- <h4 style="color:#5a6268;">under-review</h4> -->
          <hr>
          <h6>
            <a href="https://github.com/Rao-Sanaullah" target="_blank">Sanaullah</a><sup>1*</sup>,
			<a href="#top">Francisco Barranco</a><sup>2</sup>,
			<a href="#top">Eduardo Ros</a><sup>2</sup>,
			<a href="#top">Ulrich Rückert</a><sup>3</sup>,
            <a href="#top">Thorsten Jungeblut</a><sup>1</sup>
          </h6>
          * Corresponding Author: Sanaullah
          <p>
			<sup>1</sup>Industrial Internet of Things, Hochschule Bielefeld, Germany &nbsp;&nbsp;
            <sup>2</sup>CVR-Lab, University of Granada, Spain &nbsp;&nbsp;
			<sup>3</sup>Cognitronics & Sensor Systems, Universität Bielefeld, Germany
            <br>

          <div class="row justify-content-center">
            <div class="column">
              <p class="mb-5"><a class="btn btn-large btn-light"
                  href="#"
                  role="button" target="_blank">
                  <i class="fa fa-file"></i> Paper (under-review)</a> </p>
            </div>

            <div class="column">
              <p class="mb-5"><a class="btn btn-large btn-light" href="https://github.com/Rao-Sanaullah/neurocomputing_application_code"
                  role="button" target="_blank">
                  <i class="fa fa-github-alt"></i> Code</a> </p>
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>


<div class="container">
    <div class="row">
	    <section class="hand-crafted">
			<h3 class="w3l_header w3_agileits_header"><span>Abstract</span></h3>
			<div class="col-md-12 wthree_services_grid_left">
                 <p style="text-align: justify;">
		As part of an external lab collaboration (Dataninja project) with Prof. Francisco Barranco and Prof. Eduardo Ros at the Computer Vision and Robotics (CVR) Lab, 
		I worked on an event-based activity recognition dataset developed by the host lab. Our research focuses on Spiking Neural Network (SNN) exploration, where data must be represented as spike trains for effective network processing. 
		To address this issue, I designed and implemented an algorithm that converts the original event-based dataset into spike train format, tailored for SNN applications. In addition to data conversion, 
		I worked on a framework for visualizing and comparing both the original event-based dataset and the generated spike train dataset. This framework allows for the event and spike reconstruction into frame-based visualizations, 
		producing event-based videos from real and spike-based videos from generated datasets. This visualization is essential for validating the fidelity of the spike train dataset by comparing it to the original data. 
		The spike train dataset, along with the visualization framework, is publicly available on our GitHub channel, contributing valuable resources to the SNN research community and advancing the field of event-based vision exploration.</p>
			</div>
		</section>
	</div>
</div>


<!-- WHAT IS RAVSIM OFFERING -->

	<div class="container">
		<div class="col-md-12 wthree_services_grid_right">
			<div class="col-md-12 ">
		
				<div class="clearfix"> </div>
				<br><br>
				<div class="col-md-12 wthree_services_grid_left">
					<h3 class="w3l_header w3_agileits_header">What is <span>RAVSim</span> Offering</h3>
					<p style="text-align: justify;"> 
						RAVSim offers both deterministic (by solving Ordinary Differential Equations (ODEs) and stochastic (using stochastic leaky integrate-and-fire (LIF) neurons 
						algorithm) simulations. This dual simulation capability enables users to gain a comprehensive understanding of spiking neural network dynamics through various 
						approaches. What sets RAVSim apart from other tools is its unique ability to execute, analyze, extract, and validate models using image-based datasets. 
						This empowers users not only to analyze and simulate spiking neural networks but also to create custom datasets tailored to their specific image pixel, quality, 
						and extension preferences.
					</p>

					<p style="text-align: justify;"> 
						<div class="col-md-5 wthree_services_grid_left">
							<h3>A Run-time <span>VI</span> </h3>
							<p style="text-align: justify;"> 
								A run-time VI offers three different WTA Networks, List of conditions: <br>
								1- Fully connected network <br>
								2- source != target index neurons <br>
								3- source == target index neurons <br>
								Also, a mixed signal plot for a better understanding of neuron communication.
							</p>
						</div>
						
						<div class="col-md-2 wthree_services_grid_right">
							<div class="columns is-centered">
								<img style="width: 100%;" src="videos/event_event.gif">
							</div>
						</div>
						<br>					
					</p>

					<p style="text-align: justify;"> 
						<div class="col-md-7 wthree_services_grid_left">
							<br>
							<h3>Dataset Preprocessing <span>VI</span> </h3>
							<p style="text-align: justify;"> 	 
								Within dataset preprocess VI, users are allowed to use downloaded images for creating a scalable dataset. 
								This flexibility includes the ability to use images with varying extensions, sizes, and qualities, pixel to the user's specific requirements. 
								The tool supports a wide variety of image formats, accommodating datasets that suit diverse research and application needs.
							</p>							
						</div>
						<br>
						<div class="col-md-5 wthree_services_grid_right">
							<br>
							<div class="columns is-centered">
								<img style="width: 100%;" src="images/4.jpg">
							</div>
						</div>
						<br>
					</p>

					<p style="text-align: justify;"> 
						<div class="col-md-7 wthree_services_grid_left">
							<br>
							<h3>Image Classification <span>VI</span> </h3>
							<p style="text-align: justify;"> 		
								RAVSim has recently introduced new features to enhance the user experience. It now offers a convenient option to generate 
								weights for your spiking neural network models using image classification training. This feature eliminates the need for complex coding; you can simply utilize 
								RAVSim to train your model and generate the required weights effortlessly.
							</p>	
						</div>
						<br>
						<div class="col-md-5 wthree_services_grid_right">
							<br>
							<div class="columns is-centered">
								<img style="width: 100%;" src="images/2.png">
							</div>
						</div>
						<br>
					</p>
				
					<p style="text-align: justify;"> 
						<div class="col-md-7 wthree_services_grid_left">
							<br>
							<h3>SNN Models Comparison <span>VI</span> </h3>
							<p style="text-align: justify;"> 	
								In addition to weight generation, RAVSim facilitates the comparison of different spiking neural network models. Users can dynamically update sets of parameter 
								values for each model at runtime and obtain comparative results. This functionality aids in identifying the most suitable model for your specific spiking neural 
								network application.
							</p>
						</div>
						<br>
						<div class="col-md-5 wthree_services_grid_right">
							<br>
							<br>
							<div class="columns is-centered">
								<img style="width: 100%;" src="images/3.png">
							</div>
						</div>
						<br>
					</p>
				</div>
			</div>
		</div>
	</div>




    <!-- row-2 option-2 (mesh & fg rendering) -->
    <div class="d-flex justify-content-center align-items-end">
        <!-- mesh -->
        <div class="col-lg-4 col-md-6 col-sm-12 text-center">
            <img src="images/frame1.png" alt="input" class="img-fluid">
        </div>
        <!-- rendering (video) -->
        <div class="col-lg-4 col-md-6 col-sm-12 text-center">
            <div class="embed-responsive embed-responsive-16by9">

			<p " playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" id="header_vid">
              <img src="videos/event_event.gif" >
            </p>
		
			




<footer class="text-center" style="margin-bottom:10px; font-size: medium;">
  <hr>
<p>© 2023 IIOT Lab - Hochschule Bielefeld (HSBI). All Rights Reserved | Design by <a href="https://github.com/Rao-Sanaullah">Sanaullah</a></p>
</footer>

</body>

</html>
